{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "sys.path.insert(1, '../modules')\n",
    "import data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief outline of what this notebook should look like eventually:\n",
    "1. load the annotations\n",
    "2. filter out maybes\n",
    "3. load the embeddings into the list based on what's in the annotation list (so both have the same size)\n",
    "4. cross validation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in \\\n",
    "    os.listdir('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings'):\n",
    "        for file in os.listdir\\\n",
    "        ('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/' + folder):\n",
    "            timestamps.append(file.split('.')[0])\n",
    "            data = \\\n",
    "            np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "                    + folder + '/' + file)\n",
    "            emb = data['embedding']\n",
    "            embedding_list.append(emb)\n",
    "            data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(embedding):\n",
    "    embedding_mean = np.mean(embedding, axis=0)\n",
    "    embedding_std = np.std(embedding, axis=0)\n",
    "    embedding_max = np.amax(embedding, axis=0)\n",
    "    embedding_min = np.amin(embedding, axis=0)\n",
    "    embedding_summary = np.concatenate((embedding_mean, embedding_max, embedding_min, embedding_std))\n",
    "    return embedding_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_embeddings_train(embedding_list):\n",
    "#     print(embedding_list.shape)\n",
    "\n",
    "\n",
    "    expanded_embedding_list = \\\n",
    "    np.reshape(embedding_list, (embedding_list.shape[0]*embedding_list.shape[1], 512))\n",
    "    \n",
    "    #should be something like (28937, 512)\n",
    "#     print(expanded_embedding_list.shape)\n",
    "    \n",
    "    pca_45 = sklearnPCA(45)\n",
    "    pca_45.fit(expanded_embedding_list)\n",
    "    \n",
    "    embedding_list = np.asarray([pca_45.transform(embedding) for embedding in embedding_list])\n",
    "    \n",
    "    #should be something like (1523, 19, 45)\n",
    "#     print(embedding_list.shape)\n",
    "    \n",
    "    embedding_summaries = np.asarray([get_summary(embedding) for embedding in embedding_list])\n",
    "\n",
    "#     print(embedding_summaries.shape)\n",
    "    \n",
    "    return(embedding_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_embeddings_test(embedding_list):\n",
    "#     print(embedding_list.shape)\n",
    "\n",
    "\n",
    "    expanded_embedding_list = \\\n",
    "    np.reshape(embedding_list, (embedding_list.shape[0]*embedding_list.shape[1], 512))\n",
    "    \n",
    "    #should be something like (28937, 512)\n",
    "#     print(expanded_embedding_list.shape)\n",
    "    \n",
    "    pca_45.fit(expanded_embedding_list)\n",
    "    \n",
    "    embedding_list = np.asarray([pca_45.transform(embedding) for embedding in embedding_list])\n",
    "    \n",
    "    #should be something like (1523, 19, 45)\n",
    "#     print(embedding_list.shape)\n",
    "    \n",
    "    embedding_summaries = np.asarray([get_summary(embedding) for embedding in embedding_list])\n",
    "\n",
    "#     print(embedding_summaries.shape)\n",
    "    \n",
    "    return(embedding_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining embeddings with timestamp label\n",
    "NOTE: 1509 is the accurate number of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.asarray(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 180)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_summaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_embedding_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(timestamps)):\n",
    "    labeled_embedding_summaries.append([timestamps[i], embedding_summaries[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_embedding_summaries = np.asarray([[int(timestamp), embedding] for [timestamp, embedding] in labeled_embedding_summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_embedding_summaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1573060932"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_embedding_summaries[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_embedding_summaries[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_sorted = np.asarray(sorted(labeled_embedding_summaries, key=lambda tup: tup[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide annotations by day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Y list of classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/annotation_list.pickle', \"rb\") as f:\n",
    "       annotation_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list = [(int(timestamp), annotation) for (timestamp, annotation) in annotation_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1573060932, 'n')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting timestamps that aren't in labeled_embedding_summaries, so both arrays have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_cut = []\n",
    "for annotation in annotation_list:\n",
    "    if annotation[0] in labeled_embedding_summaries[:,0]:\n",
    "        annotation_list_cut.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1573063463, 'n')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_cut[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort annotation list by timestamp, then filter out anything that's not a yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_sorted = sorted(annotation_list_cut, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1573060932, 'n')"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_sorted = \\\n",
    "np.asarray([annotation for annotation in annotation_list_sorted if (annotation[1] == 'y' or annotation[1] == 'n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1573060932', '1573061031', '1573061130', ..., '1573852939',\n",
       "       '1573853129', '1573853688'], dtype='<U21')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_sorted[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1509 is the number of annotations excluding \"maybes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_day = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_list = [6, 7, 8, 9, 10, 11, 12, 13, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_day = day_list[0]\n",
    "for day in day_list:\n",
    "#     print(day)\n",
    "    i = 0\n",
    "    current_day_list = []\n",
    "#     while data.convert_timestamps(annotation_list_sorted[i][0]).day == day:\n",
    "#         current_day = data.convert_timestamps(annotation_list_sorted[i][0]).day\n",
    "#         current_day_list.append(annotation_list_sorted[i])\n",
    "#         i += 1\n",
    "    for annotation in annotation_list_sorted:\n",
    "        if data.convert_timestamps(annotation[0]).day == day:\n",
    "            current_day_list.append(annotation)\n",
    "        \n",
    "    annotation_list_day.append(current_day_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_day = np.asarray(annotation_list_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(current_day_list)\n",
    "len(annotation_list_day[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of annotations grouped by day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting embeddings by day, filtered to make sure we only have embeddings corresponding to yes/no annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_day = []\n",
    "timestamps_day = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in \\\n",
    "    os.listdir('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings'):\n",
    "    day_arr = []\n",
    "    day_time_arr = []\n",
    "    for file in os.listdir\\\n",
    "        ('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/' + folder):\n",
    "#         print(int(file.split('.')[0]))\n",
    "        if file.split('.')[0] in annotation_list_sorted[:,0]:\n",
    "            day_time_arr.append(file.split('.')[0])\n",
    "            data = \\\n",
    "            np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "                    + folder + '/' + file)\n",
    "            emb = data['embedding'] \n",
    "            day_arr.append(emb)\n",
    "            data.close()\n",
    "    timestamps_day.append(np.asarray(day_time_arr))\n",
    "    embeddings_day.append(np.asarray(day_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_day = np.asarray(embeddings_day)\n",
    "timestamps_day = np.asarray(timestamps_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeddings = 1509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=9)\n",
    "kf.get_n_splits(embeddings_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every train/test split, run PCA, get embedding summaries, use standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 2 3 4 5 6 7 8] TEST: [0]\n",
      "(88, 19, 512)\n",
      "(88, 180)\n",
      "TRAIN: [0 2 3 4 5 6 7 8] TEST: [1]\n",
      "(199, 19, 512)\n",
      "(199, 180)\n",
      "TRAIN: [0 1 3 4 5 6 7 8] TEST: [2]\n",
      "(160, 19, 512)\n",
      "(160, 180)\n",
      "TRAIN: [0 1 2 4 5 6 7 8] TEST: [3]\n",
      "(107, 19, 512)\n",
      "(107, 180)\n",
      "TRAIN: [0 1 2 3 5 6 7 8] TEST: [4]\n",
      "(115, 19, 512)\n",
      "(115, 180)\n",
      "TRAIN: [0 1 2 3 4 6 7 8] TEST: [5]\n",
      "(182, 19, 512)\n",
      "(182, 180)\n",
      "TRAIN: [0 1 2 3 4 5 7 8] TEST: [6]\n",
      "(190, 19, 512)\n",
      "(190, 180)\n",
      "TRAIN: [0 1 2 3 4 5 6 8] TEST: [7]\n",
      "(215, 19, 512)\n",
      "(215, 180)\n",
      "TRAIN: [0 1 2 3 4 5 6 7] TEST: [8]\n",
      "(253, 19, 512)\n",
      "(253, 180)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(embeddings_day):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = embeddings_day[train_index], embeddings_day[test_index]\n",
    "    y_train, y_test = annotation_list_day[train_index], annotation_list_day[test_index]\n",
    "    \n",
    "    total_train= []\n",
    "    for day in X_train:\n",
    "        for embedding in day:\n",
    "            total_train.append(embedding)\n",
    "     \n",
    "    total_train = np.asarray(total_train)\n",
    "    \n",
    "    total_train_annotations = []\n",
    "    for day in y_train:\n",
    "        for annotation in day:\n",
    "            total_train_annotations.append(annotation[1])\n",
    "            \n",
    "    total_train_annotations = np.asarray(total_train_annotations)\n",
    "    \n",
    "    #manipulate embeddings for each X_train\n",
    "    expanded_total_train = \\\n",
    "    np.reshape(total_train, (total_train.shape[0]*total_train.shape[1], 512))\n",
    "    \n",
    "    pca_45 = sklearnPCA(45)\n",
    "    pca_45.fit(expanded_total_train)\n",
    "    \n",
    "    total_train = np.asarray([pca_45.transform(embedding) for embedding in total_train])\n",
    "\n",
    "    total_train_summaries = np.asarray([get_summary(embedding) for embedding in total_train])\n",
    "\n",
    "    #standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(total_train_summaries)\n",
    "    total_train_scaler = scaler.transform(total_train_summaries)\n",
    "\n",
    "    #random forest on training data\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(total_train_scaler, total_train_annotations)\n",
    "    \n",
    "    #now apply to test data!\n",
    "    test = np.asarray(X_test[0])\n",
    "    print(test.shape)\n",
    "    \n",
    "    #manipulate embeddings for each X_test\n",
    "    expanded_test = \\\n",
    "    np.reshape(test, (test.shape[0]*test.shape[1], 512))\n",
    "    \n",
    "    test = np.asarray([pca_45.transform(embedding) for embedding in test])\n",
    "\n",
    "    test_summaries = np.asarray([get_summary(embedding) for embedding in test])\n",
    "\n",
    "    print(test_summaries.shape)\n",
    "    \n",
    "    #standard scaler on test data\n",
    "    test_scaler = scaler.transform(test_summaries)\n",
    "    \n",
    "    #random forest on test data\n",
    "    predictions = clf.predict(test_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting over, using Leave One Group Out method instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y list is just the annotations\n",
    "y = annotation_list_sorted[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_cut = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in \\\n",
    "    os.listdir('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings'):\n",
    "        for file in os.listdir\\\n",
    "        ('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/' + folder):\n",
    "            if(file.split('.')[0]) in annotation_list_sorted[:,0]:\n",
    "                data = \\\n",
    "                np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "                    + folder + '/' + file)\n",
    "                emb = data['embedding']\n",
    "                embeddings_cut.append(emb)\n",
    "#             timestamps.append(file.split('.')[0])\n",
    "#             data = \\\n",
    "#             np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "#                     + folder + '/' + file)\n",
    "#             emb = data['embedding']\n",
    "#             embedding_list.append(emb)\n",
    "#             data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(embeddings_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509, 19, 512)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.asarray([data.convert_timestamps(int(timestamp)).day for timestamp in annotation_list_sorted[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo.get_n_splits(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo.get_n_splits(groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf score:  0.6363636363636364\n",
      "precision, recall, f score:  (0.813953488372093, 0.5294117647058824, 0.44126984126984126, None)\n",
      "clf score:  0.5879396984924623\n",
      "precision, recall, f score:  (0.4260471204188482, 0.4880271243907608, 0.39184555754323197, None)\n",
      "clf score:  0.6\n",
      "precision, recall, f score:  (0.4692144373673036, 0.4976272295859925, 0.3891672631830112, None)\n",
      "clf score:  0.6261682242990654\n",
      "precision, recall, f score:  (0.46785714285714286, 0.49131274131274133, 0.4278074866310161, None)\n",
      "clf score:  0.8347826086956521\n",
      "precision, recall, f score:  (0.43636363636363634, 0.4752475247524752, 0.4549763033175355, None)\n",
      "clf score:  0.6703296703296703\n",
      "precision, recall, f score:  (0.4857142857142857, 0.4975438596491228, 0.43124999999999997, None)\n",
      "clf score:  0.6631578947368421\n",
      "precision, recall, f score:  (0.6648250460405156, 0.5325809974517656, 0.4736842105263158, None)\n",
      "clf score:  0.6837209302325581\n",
      "precision, recall, f score:  (0.5604395604395604, 0.508874546187979, 0.4450349225630124, None)\n",
      "clf score:  0.5810276679841897\n",
      "precision, recall, f score:  (0.43388429752066116, 0.48853569567483063, 0.3922679477882523, None)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in logo.split(X, groups=groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    expanded_X_train = \\\n",
    "    np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], 512))\n",
    "  \n",
    "    pca_45 = sklearnPCA(45)\n",
    "    pca_45.fit(expanded_X_train)\n",
    "   \n",
    "    X_transformed = np.asarray([pca_45.transform(embedding) for embedding in X_train])\n",
    "\n",
    "    X_summaries = np.asarray([get_summary(embedding) for embedding in X_transformed])\n",
    "    \n",
    "    #standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_summaries)\n",
    "    X_scaler = scaler.transform(X_summaries)\n",
    "\n",
    "    #random forest on training data\n",
    "    clf = RandomForestClassifier().fit(X_scaler, y_train)\n",
    "    \n",
    "    #manipulate embeddings for each X_test\n",
    "    expanded_X_test = \\\n",
    "    np.reshape(X_test, (X_test.shape[0]*X_test.shape[1], 512))\n",
    "    \n",
    "    X_test = np.asarray([pca_45.transform(embedding) for embedding in X_test])\n",
    "\n",
    "    X_test_summaries = np.asarray([get_summary(embedding) for embedding in X_test])\n",
    "    \n",
    "    #standard scaler on test data\n",
    "    X_test_scaler = scaler.transform(X_test_summaries)\n",
    "    \n",
    "    #get cross validation scores\n",
    "    print('clf score: ', clf.score(X_test_scaler, y_test))\n",
    "    \n",
    "    #get f score, precision, recall\n",
    "    #note: what average should I use???\n",
    "    y_predicted = clf.predict(X_test_scaler)\n",
    "    print\\\n",
    "    ('precision, recall, f score: ', precision_recall_fscore_support(y_test, y_predicted, average='macro'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "truck_update",
   "language": "python",
   "name": "truck_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
