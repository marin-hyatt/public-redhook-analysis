{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "#classifications\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "sys.path.insert(1, '../modules')\n",
    "import data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief outline of what this notebook should look like eventually:\n",
    "1. load the annotations\n",
    "2. filter out maybes\n",
    "3. load the embeddings into the list based on what's in the annotation list (so both have the same size)\n",
    "4. cross validation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(embedding):\n",
    "    embedding_mean = np.mean(embedding, axis=0)\n",
    "    embedding_std = np.std(embedding, axis=0)\n",
    "    embedding_max = np.amax(embedding, axis=0)\n",
    "    embedding_min = np.amin(embedding, axis=0)\n",
    "    embedding_summary = np.concatenate((embedding_mean, embedding_max, embedding_min, embedding_std))\n",
    "    return embedding_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Y list of classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide annotations by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/annotation_list.pickle', \"rb\") as f:\n",
    "       annotation_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list = np.asarray([(int(timestamp), annotation) for (timestamp, annotation) in annotation_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1573060932', 'n'], dtype='<U21')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort annotation list by timestamp, then filter out anything that's not a yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_sorted = sorted(annotation_list, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1573060932', 'n'], dtype='<U21')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_sorted = \\\n",
    "np.asarray([annotation for annotation in annotation_list_sorted if (annotation[1] == 'y' or annotation[1] == 'n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1573060932', '1573061031', '1573061130', ..., '1573852939',\n",
       "       '1573853129', '1573853688'], dtype='<U21')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_sorted[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in \\\n",
    "    os.listdir('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings'):\n",
    "        for file in os.listdir\\\n",
    "        ('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/' + folder):\n",
    "            if(file.split('.')[0]) in annotation_list_sorted[:,0]:\n",
    "                data = \\\n",
    "                np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "                    + folder + '/' + file)\n",
    "                emb = data['embedding']\n",
    "                embeddings.append(emb)\n",
    "                timestamps.append(file.split('.')[0])\n",
    "#             data = \\\n",
    "#             np.load('/green-projects/project-sonyc_redhook/workspace/share/redhook-analysis/output/embeddings/'\\\n",
    "#                     + folder + '/' + file)\n",
    "#             emb = data['embedding']\n",
    "#             embedding_list.append(emb)\n",
    "#             data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509, 19, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting timestamps that aren't in embeddings, so both arrays have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list_cut = np.asarray([annotation for annotation in annotation_list if annotation[0] in timestamps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1573063463', 'n'], dtype='<U21')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list_cut[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1509 is the number of annotations excluding \"maybes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Leave One Group Out method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y list is just the annotations\n",
    "y = annotation_list_cut[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing y and n with 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = np.where(y=='n', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 0 0 1]\n",
      "['n' 'n' 'y' 'y' 'y' 'n' 'n' 'n' 'n' 'y']\n"
     ]
    }
   ],
   "source": [
    "print(y_binary[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.asarray([data.convert_timestamps(int(timestamp)).day for timestamp in annotation_list_cut[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf score:  0.6136363636363636\n",
      "precision, recall, f score:  (0.5581395348837209, 0.5054466230936819, 0.4063492063492063, None)\n",
      "clf score:  0.6130653266331658\n",
      "precision, recall, f score:  (0.8055555555555556, 0.5064102564102564, 0.3919685726756875, None)\n",
      "clf score:  0.60625\n",
      "precision, recall, f score:  (0.303125, 0.5, 0.377431906614786, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cusp/meh708/.conda/envs/truck_update/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf score:  0.6542056074766355\n",
      "precision, recall, f score:  (0.5800970873786409, 0.5127413127413127, 0.44184407161990696, None)\n",
      "clf score:  0.8782608695652174\n",
      "precision, recall, f score:  (0.4391304347826087, 0.5, 0.4675925925925926, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cusp/meh708/.conda/envs/truck_update/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf score:  0.6538461538461539\n",
      "precision, recall, f score:  (0.3380681818181818, 0.476, 0.39534883720930225, None)\n",
      "clf score:  0.6368421052631579\n",
      "precision, recall, f score:  (0.4475806451612903, 0.4952675646159447, 0.4024340216053603, None)\n",
      "clf score:  0.6883720930232559\n",
      "precision, recall, f score:  (0.5950704225352113, 0.5040843081887858, 0.4216949696896704, None)\n",
      "clf score:  0.5849802371541502\n",
      "precision, recall, f score:  (0.4654218533886584, 0.4934861907243356, 0.4018104438289536, None)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in logo.split(X, groups=groups):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    expanded_X_train = \\\n",
    "    np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], 512))\n",
    "  \n",
    "    pca_45 = sklearnPCA(45)\n",
    "    pca_45.fit(expanded_X_train)\n",
    "   \n",
    "    X_transformed = np.asarray([pca_45.transform(embedding) for embedding in X_train])\n",
    "\n",
    "    X_summaries = np.asarray([get_summary(embedding) for embedding in X_transformed])\n",
    "    \n",
    "    #standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_summaries)\n",
    "    X_scaler = scaler.transform(X_summaries)\n",
    "\n",
    "    #random forest on training data\n",
    "    clf = RandomForestClassifier().fit(X_scaler, y_train)\n",
    "    \n",
    "    #manipulate embeddings for each X_test\n",
    "    expanded_X_test = \\\n",
    "    np.reshape(X_test, (X_test.shape[0]*X_test.shape[1], 512))\n",
    "    \n",
    "    X_test = np.asarray([pca_45.transform(embedding) for embedding in X_test])\n",
    "\n",
    "    X_test_summaries = np.asarray([get_summary(embedding) for embedding in X_test])\n",
    "    \n",
    "    #standard scaler on test data\n",
    "    X_test_scaler = scaler.transform(X_test_summaries)\n",
    "    \n",
    "    #get cross validation scores\n",
    "    print('clf score: ', clf.score(X_test_scaler, y_test))\n",
    "    \n",
    "    #get f score, precision, recall\n",
    "    #note: what average should I use???\n",
    "    y_predicted = clf.predict(X_test_scaler)\n",
    "    print\\\n",
    "    ('precision, recall, f score: ', precision_recall_fscore_support(y_test, y_predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make class to do pca with summaries\n",
    "class pca_with_summaries(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, pca):\n",
    "        self.pca = pca\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        expanded_X = np.reshape(X, (X.shape[0]*X.shape[1], 512))\n",
    "\n",
    "        self.pca.fit(expanded_X)\n",
    "        \n",
    "        return self\n",
    "  \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = np.asarray([self.pca.transform(embedding) for embedding in X])\n",
    "\n",
    "        X_summaries = np.asarray([get_summary(embedding) for embedding in X_transformed])\n",
    "        \n",
    "        return X_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('pca with summaries', pca_with_summaries(sklearnPCA(45))),\n",
    "    ('standard scaler', StandardScaler()),\n",
    "    ('random forest', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['f1', 'precision', 'recall']\n",
    "scores = cross_validate(pipeline, X, y_binary, scoring=scoring, cv=LeaveOneGroupOut(), groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([8.54602718, 6.50821447, 7.07532597, 8.43795085, 7.82813549,\n",
      "       7.45513868, 6.5654788 , 6.81613398, 6.595891  ]), 'score_time': array([0.23575258, 0.64273477, 0.19839716, 0.40478587, 0.28096747,\n",
      "       0.22913241, 0.3909905 , 0.57481408, 0.61413431]), 'test_f1': array([0.05405405, 0.02469136, 0.03030303, 0.04878049, 0.        ,\n",
      "       0.        , 0.02777778, 0.05633803, 0.08403361]), 'test_precision': array([0.33333333, 0.33333333, 0.33333333, 0.25      , 0.        ,\n",
      "       0.        , 0.2       , 0.5       , 0.27777778]), 'test_recall': array([0.02941176, 0.01282051, 0.01587302, 0.02702703, 0.        ,\n",
      "       0.        , 0.01492537, 0.02985075, 0.04950495])}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [RandomForestClassifier(), KNeighborsClassifier(), LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\u001b[0m\n",
      "{'fit_time': array([8.12407041, 6.27114987, 6.74722505, 7.01042747, 7.51113319,\n",
      "       7.1978333 , 6.63905334, 6.51876903, 6.5549562 ]), 'score_time': array([0.18472219, 0.4958334 , 0.32309985, 0.28850818, 0.27084851,\n",
      "       0.45472217, 0.47264504, 0.58437204, 0.65323162]), 'test_f1': array([0.        , 0.        , 0.        , 0.05263158, 0.        ,\n",
      "       0.03278689, 0.        , 0.        , 0.05555556]), 'test_precision': array([0.        , 0.        , 0.        , 1.        , 0.        ,\n",
      "       0.25      , 0.        , 0.        , 0.42857143]), 'test_recall': array([0.        , 0.        , 0.        , 0.02702703, 0.        ,\n",
      "       0.01754386, 0.        , 0.        , 0.02970297])}\n",
      "\u001b[1mKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\u001b[0m\n",
      "{'fit_time': array([6.70720267, 4.96011043, 5.6733079 , 5.40455437, 5.64634991,\n",
      "       5.65785003, 4.39508176, 4.64357495, 4.49196982]), 'score_time': array([0.22186637, 0.67879939, 0.60083199, 0.2509408 , 0.50602365,\n",
      "       0.62393904, 0.4844861 , 0.52775812, 0.71759367]), 'test_f1': array([0.25      , 0.28571429, 0.26804124, 0.2       , 0.20408163,\n",
      "       0.27659574, 0.29411765, 0.22033898, 0.28248588]), 'test_precision': array([0.31818182, 0.34545455, 0.38235294, 0.38461538, 0.14285714,\n",
      "       0.35135135, 0.42857143, 0.25490196, 0.32894737]), 'test_recall': array([0.20588235, 0.24358974, 0.20634921, 0.13513514, 0.35714286,\n",
      "       0.22807018, 0.2238806 , 0.19402985, 0.24752475])}\n",
      "\u001b[1mLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\u001b[0m\n",
      "{'fit_time': array([6.15156937, 5.41349912, 5.52196908, 5.86044884, 5.58823657,\n",
      "       5.91343951, 5.73791671, 5.03254175, 4.77814269]), 'score_time': array([0.16607547, 0.47435355, 0.36287117, 0.2135253 , 0.27439737,\n",
      "       0.38850904, 0.60239148, 0.32215667, 0.63310337]), 'test_f1': array([0.35714286, 0.21666667, 0.3030303 , 0.25352113, 0.22857143,\n",
      "       0.24444444, 0.23404255, 0.26785714, 0.38297872]), 'test_precision': array([0.45454545, 0.30952381, 0.41666667, 0.26470588, 0.19047619,\n",
      "       0.33333333, 0.40740741, 0.33333333, 0.4137931 ]), 'test_recall': array([0.29411765, 0.16666667, 0.23809524, 0.24324324, 0.28571429,\n",
      "       0.19298246, 0.1641791 , 0.2238806 , 0.35643564])}\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    pipeline_classifier = classifier\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "    ('pca with summaries', pca_with_summaries(sklearnPCA(45))),\n",
    "    ('standard scaler', StandardScaler()),\n",
    "    ('classifier', pipeline_classifier)\n",
    "    ])\n",
    "    \n",
    "    scoring = ['f1', 'precision', 'recall']\n",
    "    scores = cross_validate(pipeline, X, y_binary, scoring=scoring, cv=LeaveOneGroupOut(), groups=groups)\n",
    "    \n",
    "    print('\\033[1m' + str(classifier) + '\\033[0m')\n",
    "    \n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "truck_update",
   "language": "python",
   "name": "truck_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
